{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ PoC AI Pool 2025 ~\n",
    "- ## Day 2: Neural Networks from Scratch\n",
    "    - ### Module 2: Logistic Regression\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you just dove into linear regression; let's discover another banger — **logistic regression**! \n",
    "\n",
    "While linear regression outputs continuous values, logistic regression predicts probabilities, making it ideal for classification tasks.\n",
    "\n",
    "The key difference lies in the **output function**:\n",
    "- Linear regression: $$y = a * x + b$$\n",
    "- Logistic regression: $$ y = sigmoid(a * x + b)$$\n",
    "\n",
    "Moreover, you might wonder: is it possible to perform logistic regression with a polynomial function? The answer is **yes**! Logistic regression can work with polynomial transformations of the input, allowing the model to capture non-linear decision boundaries.\n",
    "\n",
    "Let's dive into building logistic regression step by step, including polynomial transformations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training set\n",
    "train_set = [\n",
    "    [1, 0],\n",
    "    [2, 0],\n",
    "    [3, 0],\n",
    "    [4, 0],\n",
    "    [5, 0],\n",
    "    [6, 1],\n",
    "    [7, 1],\n",
    "    [8, 1],\n",
    "    [9, 1],\n",
    "    [10, 1]\n",
    "]\n",
    "\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why a dataset like this you will say ? Because logistic regression works well as find cluster of data and make a linear observation of it here's what you need to understand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the training set into two clusters based on the label\n",
    "cluster_0 = [point[0] for point in train_set if point[1] == 0]\n",
    "cluster_1 = [point[0] for point in train_set if point[1] == 1]\n",
    "\n",
    "# Plot the clusters\n",
    "plt.scatter(cluster_0, [0] * len(cluster_0), color='red', label='Cluster 0')\n",
    "plt.scatter(cluster_1, [1] * len(cluster_1), color='blue', label='Cluster 1')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Cluster')\n",
    "plt.legend()\n",
    "plt.title('Training Set Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the Weight and Bias\n",
    "\n",
    "We need to initialize both `w` and `b` with random values beetwen 0 and 10 at the start :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "#TODO: randomise the weight and bias beetwen 0 and 10\n",
    "\n",
    "mini = 0\n",
    "maxi = 10\n",
    "\n",
    "w = ...\n",
    "b = ...\n",
    "\n",
    "print(\"Initial Weight:\", w)\n",
    "print(\"Initial Bias:\", b)\n",
    "\n",
    "assert w > 8.44 and w < 8.45, \"Weight is not correct\"\n",
    "assert b > 7.57 and b < 7.58, \"Bias is not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3 : Make prediction using the known formula (with sigmoid)\n",
    "\n",
    "To make predictions, we’ll use the sigmoid function, a fundamental tool in machine learning. The sigmoid is often used to squash values into the range [0, 1], which makes it particularly useful for binary classification tasks. It’s defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(y) = \\frac{1}{1 + e^{-y}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "•\t $ y = w \\cdot x + b $  (the neuron formula if you had forgotten)\n",
    "\n",
    "•\t $ e $ is the base of the natural logarithm.\n",
    "\n",
    "The sigmoid function ensures that large positive values of  z  approach 1, and large negative values approach 0, with a smooth curve in between.\n",
    "\n",
    "\n",
    "With the formula of above, calculate the `y_pred` of each input `x` in the train_set in a function call **forward** and it will use your **sigmoid** function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "e = math.e\n",
    "\n",
    "#TODO: Define the sigmoid function (use pow)\n",
    "def sigmoid(x):\n",
    "    ...\n",
    "\n",
    "assert sigmoid(1) > 0.73 and sigmoid(1) < 0.74, \"Sigmoid is not correct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Define the forward function -> neuron function\n",
    "def forward(x):\n",
    "    ...\n",
    "\n",
    "#TODO: Test the forward function\n",
    "for x, y in train_set:\n",
    "    y_pred = ...\n",
    "    print(\"Prediction:\", y_pred, \"Actual:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of the prediction and the actual data\n",
    "x = [i[0] for i in train_set]\n",
    "y = [i[1] for i in train_set]\n",
    "y_pred = [forward(i) for i in x]\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(x, y_pred, 'bo')\n",
    "plt.axis([0, 11, -0.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close to having everything correct on the first try! As you can see, the separation between the two groups isn’t very clear yet. This is where the loss like before comes in to help us improve.\n",
    "\n",
    "For this case, we’re going to use a different loss function: **Binary Cross-Entropy Loss**. This loss function is specifically designed for binary classification tasks *(predicting values between 0 and 1)*, which aligns perfectly with the output of our sigmoid function. Pretty neat, right?\n",
    "\n",
    "Here’s a breakdown of how it works:\n",
    "\n",
    "---\n",
    "\n",
    "#### *Binary Cross-Entropy Loss Formula*\n",
    "\n",
    "The Binary Cross-Entropy Loss (Single prediction) is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "•\t￼ $ y $ is the true label (0 or 1) for sample ￼.\n",
    "\n",
    "•\t￼ $ \\hat{y} $ is the predicted probability for sample ￼ (the output of the function).\n",
    "\n",
    "•\t￼$\\log$ is the natural logarithm.\n",
    "\n",
    "\n",
    "The overall goal of this loss function is to minimize the difference between the true labels ￼ and the predicted probabilities ￼, guiding the model to make better predictions.\n",
    "\n",
    "Next, implement this formula in your code to calculate the loss for your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "eps = 1e-15\n",
    "\n",
    "def binary_cross_entropy_loss(y, y_pred):\n",
    "    # We clamp the prediction value to avoid log(0)\n",
    "    y_pred_clamped = max(min(y_pred, 1 - eps), eps)\n",
    "    ...\n",
    "\n",
    "#TODO: Test the binary cross entropy loss function\n",
    "for x, y in train_set:\n",
    "    ...\n",
    "    error = ...\n",
    "    print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the error for the data that are one is so small, but the first one is the one we need to update, the derivative are more simple here !\n",
    "\n",
    "The derivative for `w`\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = (y_{\\text{pred}} - y) \\, x.\n",
    "\n",
    "$$\n",
    "\n",
    "The derivative for `b`\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = (y_{\\text{pred}} - y)\n",
    "\n",
    "$$\n",
    "\n",
    "*Bonus : To better understand and apply the derivative of the function BCE with the logistic regression, try calculating it manually and never forget the chain rule ! :)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO setup the derivative of the loss function \n",
    "def derivative_w(x, y, y_pred):\n",
    "    ...\n",
    "\n",
    "def derivative_b(y, y_pred):\n",
    "    ...\n",
    "\n",
    "def derivative(x, y, y_pred):\n",
    "    return (derivative_w(x, y, y_pred), derivative_b(y, y_pred))\n",
    "\n",
    "for x, y in train_set:\n",
    "    ...\n",
    "    derivative_weight, derivative_bias = ...\n",
    "\n",
    "    print(f\"x: {x}, y: {y}, y_pred: {y_pred:.3f}, Derivative Weight: {derivative_weight:.3f}, Derivative Bias: {derivative_bias:.3f}\")\n",
    "    # Plot the derivatives\n",
    "    dw_values = [derivative_w(x, y, forward(x)) for x, y in train_set]\n",
    "    db_values = [derivative_b(y, forward(x)) for x, y in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot derivative of w\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(dw_values)), dw_values, marker='o', color='blue', label=\"Derivative of w\")\n",
    "plt.title(\"Derivative of Loss with respect to w\")\n",
    "plt.xlabel(\"Index of point in dataset\")\n",
    "plt.ylabel(\"Derivative (dL/dw)\")\n",
    "plt.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot derivative of b\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(db_values)), db_values, marker='o', color='orange', label=\"Derivative of b\")\n",
    "plt.title(\"Derivative of Loss with respect to b\")\n",
    "plt.xlabel(\"Index of point in dataset\")\n",
    "plt.ylabel(\"Derivative (dL/db)\")\n",
    "plt.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONNAL BUT RECOMMENDED : if you want reload the weight randomly at the first value \n",
    "\n",
    "w = random.uniform(mini, maxi)\n",
    "b = random.uniform(mini, maxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try your best to implement thetrain version ! *The same way you did for the linear regression remember it !*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Train the model\n",
    "\n",
    "epochs = ...\n",
    "learning_rate = ...\n",
    "\n",
    "\n",
    "for ... :\n",
    "    for ... :\n",
    "    # ~ Also 5 lines to code \n",
    "    print(f\"Epoch: {epoch}, Loss: {loss:.5f}\")\n",
    "\n",
    "print(\"Final Weight:\", w)\n",
    "print(\"Final Bias:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "print (\"Prediction \", forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_set:\n",
    "    y_pred = forward(x)\n",
    "    print (x, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the actual data points\n",
    "plt.scatter(cluster_0, [0] * len(cluster_0), color='red', label='Cluster 0')\n",
    "plt.scatter(cluster_1, [1] * len(cluster_1), color='blue', label='Cluster 1')\n",
    "\n",
    "# Plot the predicted probabilities\n",
    "x_values = range(0, 11)\n",
    "y_pred_values = [forward(x) for x in x_values]\n",
    "plt.plot(x_values, y_pred_values, color='green', linestyle='-', linewidth=2, marker='o', label='Predicted Probability')\n",
    "\n",
    "# Add a horizontal line at 0.5\n",
    "plt.axhline(y=0.5, color='grey', linestyle='--', linewidth=0.8, label='Threshold 0.5')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Logistic Regression Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Congratulations on building your second machine learning algorithm !! now let's level up the difficulty and introduce you to the concept of neural network, good luck ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
