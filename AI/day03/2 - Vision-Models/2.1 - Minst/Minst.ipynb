{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ PoC AI Pool 2025 ~\n",
    "- ## Day 3: Deep Learning\n",
    "    - ### Module 2: Convolutional Neural Network\n",
    "-----------\n",
    "\n",
    "## Minst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, you've arrived here ! You now understand key concepts of neural networks and how they are trained, but you haven't really created one yet...\n",
    "Don't worry this task will guide you in recreating a neural network trained to detect any handwritten digit on a 28 by 28 pixel image !\n",
    "\n",
    "Your will start by setup the dataset, your model and at the end, play with it ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just import the necessary libraries\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#For the model don't forget\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part - 1 Prepare the data \n",
    "\n",
    "before actually create a neural network we need to preparate our data that we will fit to your model,\n",
    "\n",
    "remember ***THE MOST important in machine learning is the quality of the data*** and not really the model....\n",
    "\n",
    "your goal here is to specify how we want the data, this can be process by initialise a data and transform it in a [tensor](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) and normalize it if you want. you can check the doc of transform [here](https://pytorch.org/vision/0.9/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: define the transforms compose\n",
    "transform = ...\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "eval_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(f\"Len train dataset : {len(train_set)}\")\n",
    "print(f\"Len test  dataset : {len(eval_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will say why whe created two dataset ? \n",
    "\n",
    "It's because one will be for the training of the model and the other for evaluate this one by passing data he never seen, to see if the model didn't overfit the data.\n",
    "\n",
    "To understand what's inside this code you can try below to visualise some of the examples !\n",
    "\n",
    "***Don't hesitate to change the NUMBER_OF_ELEMENTS enum to see mutliples examples or no***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of some element of the dataset you can change the number if you want\n",
    "NUMBER_OF_ELEMENTS = 4\n",
    "\n",
    "def imshow(img):\n",
    "    # img = img * 0.5 + 0.5  # Denormalisation if you have normalised the data\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "train_loader_vis = torch.utils.data.DataLoader(train_set, batch_size=NUMBER_OF_ELEMENTS, shuffle=True)\n",
    "\n",
    "# Random image \n",
    "dataiter = iter(train_loader_vis)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Labels :', ' '.join(f'{labels[j].item()};' for j in range(NUMBER_OF_ELEMENTS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at different attributes like the number of images in the dataset, the size of each image or the label of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_set[0]\n",
    "\n",
    "print(\"image :\", image) # pixels value if you want to see the matrix\n",
    "print(\"-\"*60)\n",
    "print(\"image shape :\", image.shape) # pixels value\n",
    "print(\"label :\", label) # Number represented in the image \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have images **28 pixels high and 28 pixels wide**, with **one channel** (grayscale !).\n",
    "\n",
    "These images represent a number from 0 to 9, we have **10 different labels** (or 10 different possible output).\\\n",
    "The first picture represents a 5, therefore its label is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Batch-Size\n",
    "\n",
    "Did you remember when we talk about batch and parallelization of multiple example with torch ? This is very important here !\n",
    "\n",
    "**60,000** is a lot of images to process one by one, to make it easier for our model to process this data while training we are going to use ``batch_size``.\n",
    "\n",
    "for one who forget , ``batch_size`` is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. In other words, before calculating the error and apply backpropagation after each image, if our batch size is 64 we will go through 64 images before doing it. **This improves the learning of our AI** by **applying the backpropagation on the error average.**\n",
    "\n",
    "As in the previous notebook we will use a [**``dataloader``**](https://pytorch.org/docs/stable/data.html), this time we don't need to redefine a ``Dataset`` class since we are using a ``builtin`` dataset in ``torchvision``.\n",
    "\n",
    "Remember to specify that you use the ``train_set`` and you want a ``batch_size`` of ``64`` and also ``shuffle`` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Define the batch size\n",
    "BATCH_SIZE = ...\n",
    "\n",
    "train_loader = ...\n",
    "\n",
    "assert len(train_loader) == 938, \"Your train loader is not well implemented, remember that the batch size is 64\"\n",
    "\n",
    "batch = next(iter(train_loader)) # obtain the first batch\n",
    "images, labels = batch\n",
    "print(\"image shape :\", images.shape)\n",
    "print(\"labels shape :\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `938` lots containing `64` images each (and their equivalent labels).\\\n",
    "This will **drastically decrease our training time** because with one backward propagation, 64 images are processed.\n",
    "\n",
    "\n",
    "> Pytorch is built to be used with batch, it is thus quite simple to implement it in our code. \n",
    "\n",
    "*you can try after to change your batch and see the difference in the learning (remove the assert for test it)* !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Also load the test set with the same batch_size...\n",
    "\n",
    "eval_loader = ...\n",
    "\n",
    "assert len(eval_loader) == 157, \"Your eval loader is not well implemented\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model!\n",
    "\n",
    "And your moment has arrived!\n",
    "\n",
    "I’m sure you’ve been eagerly anticipating this step, and now you’re ready to build your very first real neural network, complete with a more complex architecture.\n",
    "\n",
    "A quick tip for working with PyTorch: today’s task is a classification problem, as we’ve defined specific output labels. For this, we’ll be using the **[cross-entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)** loss function. (Remember, yesterday you used the **[binary cross-entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)** loss with logistic regression, since the output was restricted to just 0 or 1.)\n",
    "\n",
    "*Don't hesistate to jump at the end of the torch introduction as helping you for initialize the model and train it !*\n",
    "\n",
    "IF you encounter difficulties to create your model, at the end of this notebook there is a pseudo code of the architecture as to help you to create the model, but try to do it alone ! (with everything you see before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Define the learning rate\n",
    "LEARNING_RATE = ...\n",
    "\n",
    "\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.flatten = ...\n",
    "        self.fc1 = ... \n",
    "        #TODO : add other layers if you want\n",
    "        ...\n",
    "\n",
    "        self.loss = ... # Loss function cross entropy\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE) # Optimizer Adam\n",
    "        #TODO : add other optimizers if you want\n",
    "        self.relu = ... # Activation function\n",
    "        ...\n",
    "        \n",
    "        # Device choice \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        print(f\"Device : {self.device}\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO : Define the forward pass\n",
    "        x = ... \n",
    "        ...\n",
    "        return ...\n",
    "\n",
    "\n",
    "    def train_model(self, epochs, train_loader):\n",
    "        self.train()  # Training mode\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "            running_loss = 0.0\n",
    "            total_batches = 0\n",
    "\n",
    "            for i, data in enumerate(train_loader): # Enumerate the data, all the dataset\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                #TODO Compute the training part ~ 5 lines\n",
    "                ...\n",
    "                ###################################\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                total_batches += 1 # just help for print \n",
    "\n",
    "                # print every 8 mini-batches\n",
    "                if (i + 1) % 8 == 0 or (i + 1) == len(train_loader):\n",
    "                    print(f\"\\rEpochs {epoch + 1}/{epochs} | Lot {i + 1}/{len(train_loader)} | Loss : {loss.item():.4f}\", end='')\n",
    "\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"Epochs {epoch + 1}/{epochs} finish | Average Loss : {avg_loss:.4f} | Time : {epoch_time:.2f} seconds\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # change the model_path if you want\n",
    "        model_path = \"mnist_model.pth\"\n",
    "        print('Training finished, saving model to :', model_path)\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    def eval_model(self, test_loader):\n",
    "        self.eval()  # Evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the model on {total} images is : {100 * correct / total:.2f}%')\n",
    "\n",
    "    def load_weights(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path, weights_only=True, map_location=self.device))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done ! you need now to initialise your model by simple call your python class, \n",
    "\n",
    "It permits that if you want to restart the training with random weights, you can restart this cell. Otherwise, the training if (you restart it) will continue from the **`last loss value`** and the **`last weight`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MNISTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: define number of epochs\n",
    "EPOCHS = ...\n",
    "\n",
    "my_model.train_model(EPOCHS, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can now test your model by simply call the eval function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.eval_model(eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’d like to retrain and check for better results, simply re-run the training cell or initialize a new model to start fresh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with your model !\n",
    "\n",
    "Now it's time to test your own model! **Please paste your model architecture** (*`__init__`* and *`forward`* methods) into the file [model.py](model.py), and run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "after this break, you have two option : \n",
    "\n",
    "- ***2.2 - Cifar*** -> try to implemente an really complex architecture called VAE-GAN for another task \n",
    "\n",
    "- ***3.1 - My torch*** -> try to recreate some function of torch, to really understand how this is work (it my be help you for creating a VAE-GAN architecture :))\n",
    "\n",
    "choose one ! *(you can do both also if you finish in advance)*\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Define the learning rate\n",
    "LEARNING_RATE = ...\n",
    "\n",
    "\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.flatten = ... # Flatten the data\n",
    "        self.fc1 = ... # Fully connected layer from 28**28 to 128\n",
    "        self.fc2 = ... # Fully connected layer from 128 to 64\n",
    "        self.fc3 = ... # Fully connected layer from 64 to 10\n",
    "\n",
    "        self.loss = ... # Loss function cross entropy\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE) # Optimizer Adam\n",
    "        self.relu = ... # Activation function\n",
    "        \n",
    "        # Device choice \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        print(f\"Device : {self.device}\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = ... # Flatten the data\n",
    "\n",
    "        ... # Compute your self.fc1\n",
    "        ... # Activation function\n",
    "\n",
    "        ... # Compute your self.fc2\n",
    "        ... # Activation function\n",
    "        ... # Compute your self.fc3\n",
    "\n",
    "        return ...\n",
    "\n",
    "\n",
    "    def train_model(self, epochs, train_loader):\n",
    "        self.train()  # Training mode\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "            running_loss = 0.0\n",
    "            total_batches = 0\n",
    "\n",
    "            for i, data in enumerate(train_loader): # Enumerate the data, all the dataset\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Gradient to zero\n",
    "                ...\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = ...\n",
    "\n",
    "                # Loss calculation\n",
    "                loss = ...\n",
    "\n",
    "                # Backward pass\n",
    "                ...\n",
    "\n",
    "                # Optimisation step\n",
    "                ...\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                total_batches += 1 # just help for print \n",
    "\n",
    "                # print every 8 mini-batches\n",
    "                if (i + 1) % 8 == 0 or (i + 1) == len(train_loader):\n",
    "                    print(f\"\\rEpochs {epoch + 1}/{epochs} | Lot {i + 1}/{len(train_loader)} | Loss : {loss.item():.4f}\", end='')\n",
    "\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"Epochs {epoch + 1}/{epochs} finish | Average Loss : {avg_loss:.4f} | Time : {epoch_time:.2f} seconds\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # change the model_path if you want\n",
    "        model_path = \"mnist_model.pth\"\n",
    "        print('Training finished, saving model to :', model_path)\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    def eval_model(self, test_loader):\n",
    "        self.eval()  # Evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the model on {total} images is : {100 * correct / total:.2f}%')\n",
    "\n",
    "    def load_weights(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path, weights_only=True, map_location=self.device))\n",
    "        self.eval()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
